{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNeDlK6Sv/DHV978SnTCiTc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saraaburomoh/Fares-and-Sara/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow Datasets (if not already installed)\n",
        "!pip install tensorflow-datasets\n",
        "\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Flowers dataset\n",
        "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes\n",
        "train_ratio = 0.8\n",
        "num_train_examples = int(num_examples * train_ratio)\n",
        "num_val_examples = num_examples - num_train_examples\n",
        "train_dataset = dataset['train'].take(num_train_examples)\n",
        "val_dataset = dataset['train'].skip(num_train_examples)\n",
        "\n",
        "# Preprocess and batch the datasets\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Convert the mapped dataset to NumPy arrays\n",
        "train_images = np.array([sample[0] for sample in train_dataset.map(preprocess)])\n",
        "train_labels = np.array([sample[1] for sample in train_dataset.map(preprocess)])\n",
        "\n",
        "# Create an instance of the ImageDataGenerator class with augmentation configurations\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Apply data augmentation to the training dataset using NumPy arrays\n",
        "augmented_train_dataset = datagen.flow(train_images, train_labels, batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "qTcdQ65uLF0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create MobileNetV2 base model\n",
        "base_model = MobileNetV2(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "# Freeze all layers initially\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Unfreeze the last 10 layers for fine-tuning\n",
        "for layer in base_model.layers[-5:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Create the predictions layer\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(DENSE_UNITS, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Use a lower learning rate for fine-tuned layers\n",
        "optimizer = Adam(learning_rate=FINE_TUNING_LEARNING_RATE)\n",
        "\n",
        "# Compile the model with the updated optimizer\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0.0,\n",
        "    patience=PATIENCE,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "KDaq4bl1LGoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with early stopping\n",
        "history = model.fit(\n",
        "    augmented_train_dataset,\n",
        "    epochs=20,  # You can adjust the number of epochs\n",
        "    validation_data=val_dataset.map(preprocess).batch(BATCH_SIZE),\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "VziXSAYWLKgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(val_dataset.map(preprocess).batch(BATCH_SIZE))\n",
        "\n",
        "# Save the model\n",
        "model.save('my_model.h5')\n"
      ],
      "metadata": {
        "id": "GO4O-eURLP5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results presentation\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}\")\n"
      ],
      "metadata": {
        "id": "CXWbW6D1LTHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show multiple sample images from the dataset\n",
        "sample_images, sample_labels = next(iter(train_dataset.map(preprocess).batch(5)))  # Display 5 samples\n",
        "sample_images = sample_images.numpy()\n",
        "sample_labels = sample_labels.numpy()\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(sample_images[i])\n",
        "    plt.title(f\"Class: {info.features['label'].int2str(sample_labels[i])}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "syGUI0QELT0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}